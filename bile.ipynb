{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9f3c26d-8060-400a-a65b-decee2451277",
   "metadata": {},
   "source": [
    "# Prediction of bile predicaments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1856e1-de7f-4922-8d8c-1c653488d1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_decision_forests as tfdf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow_decision_forests as tfdf\n",
    "\n",
    "import dtreeviz\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython import display\n",
    "\n",
    "# avoid \"Arial font not found warnings\"\n",
    "import logging\n",
    "logging.getLogger('matplotlib.font_manager').setLevel(level=logging.CRITICAL)\n",
    "\n",
    "display.set_matplotlib_formats('retina') # generate hires plots\n",
    "\n",
    "np.random.seed(1234)  # reproducible plots_data for explanatory reasons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e67169-68b8-4832-a899-0ff7a008b23d",
   "metadata": {},
   "source": [
    "## Get & clean data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a331b97-50db-495a-a070-8e0f40a8e777",
   "metadata": {},
   "source": [
    "### Load and inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85455d86-013a-48b3-b3e6-12631a5d4f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_idc = pd.read_excel(\"data/Intraductal_concrement.xlsx\", sheet_name=0, skiprows=1, header=None)\n",
    "df_idc_visits = pd.read_excel(\"data/Intraductal_concrement.xlsx\", sheet_name=0, skiprows=0, header=None)\n",
    "df_idc.columns = df_idc_visits.iloc[0] + '_' + df_idc.iloc[0]\n",
    "df_idc.columns = df_idc.columns.str.lower()\n",
    "df_idc.insert(2, 'has_idc', \"1\")\n",
    "df_idc.drop(index=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ff229b-ac1c-4257-b2ab-2b58b054b97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_idc.iloc[:, 2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f405a74b-51f0-4d32-bd98-35d1ffee108b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_idc.columns.tolist()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877d5ddf-cab4-42bd-8b4f-a28c39850cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_idc = pd.read_excel(\"data/No_intraductal_concrement.xlsx\", sheet_name=0, skiprows=1, header=None)\n",
    "df_no_idc_visits = pd.read_excel(\"data/No_intraductal_concrement.xlsx\", sheet_name=0, skiprows=0, header=None)\n",
    "df_no_idc.columns = df_no_idc_visits.iloc[0] + '_' + df_no_idc.iloc[0]\n",
    "df_no_idc.columns = df_no_idc.columns.str.lower()\n",
    "df_no_idc.insert(2, 'has_idc', \"0\")\n",
    "df_no_idc.drop(index=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e974aa-f32f-4dab-9d24-15b5473c42c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_idc.iloc[:, 2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4bd7c0-cbcd-4a62-8618-d78462ef8a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_idc.columns.tolist()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5961901e-2274-4c3f-8c72-4d4899e067b4",
   "metadata": {},
   "source": [
    "### Check that column names are identical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606b01c5-dea6-4327-bbd0-d7b34f2bbcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(df_idc.columns.tolist() == df_no_idc.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bc1778-e02a-4f78-820a-fd4a6eac62d9",
   "metadata": {},
   "source": [
    "### Join the data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62903f48-0bc1-4a48-b570-f4d6f0a6621d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = pd.concat([df_idc, df_no_idc])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869f5e46-a779-4543-b616-aea73ea9dc48",
   "metadata": {},
   "source": [
    "### Clean and format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddfef1d-4c64-4935-8965-5de4e602b792",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data.replace(['unknown', 'missing', 'ns', ' ns', \"ms\", \"s\", \"sn\"], np.nan, inplace=True)\n",
    "lab_cols = input_data.filter(like='lab_values').columns\n",
    "input_data[lab_cols] = input_data[lab_cols].replace(',', '.', regex=True)\n",
    "input_data[lab_cols] = input_data[lab_cols].replace(['.*\\\\.\\\\..*', '.*\\\\.\\\\.\\\\..*'], pd.NA, regex=True)\n",
    "input_data[lab_cols] = input_data[lab_cols].replace('ÃŸ', '0', regex=True)\n",
    "input_data[lab_cols] = input_data[lab_cols].replace('.', np.nan)\n",
    "input_data[lab_cols] = input_data[lab_cols].replace('n2.23', np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data[['ercp_date', 'baseline_characteristics_admission_date']] = input_data[['ercp_date', 'baseline_characteristics_admission_date']].replace(['.*\\\\.\\\\..*', '.*\\\\.\\\\.\\\\..*'], pd.NA, regex=True)\n",
    "\n",
    "input_data = input_data.convert_dtypes()\n",
    "input_data[lab_cols] = input_data[lab_cols].astype('float64')\n",
    "\n",
    "# Add column if ercp_date was within 3 days of admission baseline_characteristics_admission_date\n",
    "input_data['ercp_date'] = pd.to_datetime(input_data['ercp_date'])\n",
    "input_data['baseline_characteristics_admission_date'] = pd.to_datetime(input_data['baseline_characteristics_admission_date'])\n",
    "input_data['ercp_date_minus_admission_date'] = input_data['ercp_date'] - input_data['baseline_characteristics_admission_date']\n",
    "input_data['ecrp_gte_3_days_naT'] = input_data['ercp_date_minus_admission_date'] >= pd.Timedelta(days=3) \n",
    "input_data['ercp_date_na'] = input_data['ercp_date'].isna()\n",
    "input_data['ecrp_gte_3_days'] =  input_data['ercp_date_na'] | input_data['ecrp_gte_3_days_naT']\n",
    "\n",
    "# Rename column names of input_data\n",
    "input_data.columns = input_data.columns.str.replace('/', '_')\n",
    "input_data.columns = input_data.columns.str.replace(' ', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec1e43e-2dc3-4636-8949-81f9c2c34b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_data.iloc[:, 2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8d4fc8-19a4-4cdb-a1f3-97ad7e848257",
   "metadata": {},
   "source": [
    "### Write data to csv for external analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da35ee7-3336-4e8d-a018-15361671615d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_tab1 = input_data.copy()\n",
    "input_data_tab1.loc[:, 'has_idc'] = input_data_tab1['has_idc'].astype('category')\n",
    "input_data_tab1.to_csv(\"data/input_data_tab1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data for TFDF: BASELINE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1ab8c1",
   "metadata": {},
   "source": [
    "### Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab354b0b-9216-442e-8a01-45ff141d289e",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_COLUMN_NAME = [\"has_idc\"]\n",
    "\n",
    "ECRP_GTE_3_DAYS = [\"ecrp_gte_3_days\"]\n",
    "\n",
    "DATA_HEADER = input_data.columns\n",
    "\n",
    "CATEGORICAL_FEATURE_NAMES = [\n",
    "                            'baseline_characteristics_sex', \n",
    "                            'baseline_characteristics_nicotine_use',\n",
    "                            #'baseline_characteristics_alcohol_use',\n",
    "                            ]\n",
    "\n",
    "NUMERIC_BASELINE_FEATURE_NAMES = [\n",
    "                            'baseline_characteristics_age',\n",
    "                            'baseline_characteristics_height_in_cm',\n",
    "                            'baseline_characteristics_weight_in_kg',\n",
    "                            # 'baseline_characteristics_bmi',\n",
    "                             #'eus_ercp_mrcp_or_as_dhc_width_in_mm'\n",
    "                            ]\n",
    "\n",
    "# NUMERIC_LAB_FEATURE_NAMES_ADMISSION = input_data.filter(like='lab_values_admission').columns.tolist()\n",
    "# Remove variables with mostly missing values\n",
    "NUMERIC_LAB_FEATURE_NAMES_ADMISSION =['lab_values_admission_crp_in_mg_dl',\n",
    "                                    #'lab_values_admission_pct_in_ng_ml',\n",
    "                                    #'lab_values_admission_il6_in_pg_ml',\n",
    "                                    'lab_values_admission_leukocytes_in_g_l',\n",
    "                                    'lab_values_admission_bilirubin_in_mg_dl',\n",
    "                                    'lab_values_admission_got_ast_in_u_l',\n",
    "                                    'lab_values_admission_gpt_alt_in_u_l',\n",
    "                                    'lab_values_admission_gamma-gt_in_u_l',\n",
    "                                    'lab_values_admission_alkaline_phosphatase_in_u_l',\n",
    "                                    #'lab_values_admission_lipase_in_u_l',\n",
    "                                    #'lab_values_admission_inr',\n",
    "                                    # 'lab_values_admission_quick _in_%',\n",
    "                                    'lab_values_admission_calcium_in_mmol_l',\n",
    "                                    # 'lab_values_admission_calcium_alb.-corrected_mmol_l',\n",
    "                                    'lab_values_admission_triglycerides_in_mg_dl',\n",
    "                                    'lab_values_admission_hematocrit',\n",
    "                                    #'lab_values_admission_gfr_in_ml_min',\n",
    "                                    'lab_values_admission_creatinine_in_mg_dl',\n",
    "                                    #'lab_values_admission_urea_in_mg_dl',\n",
    "                                    #'lab_values_admission_igg4_in_g_l'\n",
    "                                    ]\n",
    "# Fix names for use in model\n",
    "\n",
    "# NUMERIC_LAB_FEATURE_NAMES_DAY3 = input_data.filter(like='lab_values_day_3').columns.tolist()\n",
    "\n",
    "NUMERIC_LAB_FEATURE_NAMES_DAY3 = ['lab_values_day_3_crp_in_mg_dl',\n",
    "                                #'lab_values_day_3_pct_in_ng_ml',\n",
    "                                #'lab_values_day_3_il6_in_pg_ml',\n",
    "                                #'lab_values_day_3_leukocytes_in_g_l',\n",
    "                                'lab_values_day_3_bilirubin_in_mg_dl',\n",
    "                                'lab_values_day_3_got_ast_in_u_l',\n",
    "                                'lab_values_day_3_gpt_alt_in_u_l',\n",
    "                                'lab_values_day_3_gamma-gt_in_u_l',\n",
    "                                'lab_values_day_3_alkaline_phosphatase_in_u_l',\n",
    "                                #'lab_values_day_3_lipase_in_u_l',\n",
    "                                #'lab_values_day_3_inr',\n",
    "                                #'lab_values_day_3_quick _in_%',\n",
    "                                #'lab_values_day_3_calcium_in_mmol_l',\n",
    "                                #'lab_values_day_3_calcium_alb.-corrected_mmol_l',\n",
    "                                'lab_values_day_3_triglycerides_in_mg_dl',\n",
    "                                'lab_values_day_3_hematocrit',\n",
    "                                #'lab_values_day_3_gfr_in_ml_min',\n",
    "                                'lab_values_day_3_creatinine_in_mg_dl',\n",
    "                                #'lab_values_day_3_urea_in_mg_dl'\n",
    "                                ]\n",
    "\n",
    "NUMERIC_FEATURE_NAMES_ALL = NUMERIC_BASELINE_FEATURE_NAMES + NUMERIC_LAB_FEATURE_NAMES_ADMISSION + NUMERIC_LAB_FEATURE_NAMES_DAY3\n",
    "ALL_FEATURE_NAMES_ADMISSION = TARGET_COLUMN_NAME + CATEGORICAL_FEATURE_NAMES + NUMERIC_BASELINE_FEATURE_NAMES + NUMERIC_LAB_FEATURE_NAMES_ADMISSION\n",
    "ALL_FEATURE_NAMES_DAY3 = TARGET_COLUMN_NAME + CATEGORICAL_FEATURE_NAMES + NUMERIC_BASELINE_FEATURE_NAMES + NUMERIC_LAB_FEATURE_NAMES_DAY3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select and convert data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41727237-8ddb-46f4-9608-2e3903033068",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = pd.DataFrame()\n",
    "\n",
    "x_data_pre = input_data.copy()\n",
    "\n",
    "x_data[TARGET_COLUMN_NAME] = x_data_pre[TARGET_COLUMN_NAME].astype('int')\n",
    "\n",
    "x_data[CATEGORICAL_FEATURE_NAMES] = x_data_pre[CATEGORICAL_FEATURE_NAMES].fillna('').astype('category')\n",
    "if len(CATEGORICAL_FEATURE_NAMES) > 0:\n",
    "    x_data[CATEGORICAL_FEATURE_NAMES] = x_data[CATEGORICAL_FEATURE_NAMES].apply(lambda x: x.cat.codes).astype('int64')\n",
    "\n",
    "x_data[NUMERIC_FEATURE_NAMES_ALL]= x_data_pre[NUMERIC_FEATURE_NAMES_ALL].fillna(float(np.nan)).astype('float64')\n",
    "\n",
    "x_data[ECRP_GTE_3_DAYS] = x_data_pre[ECRP_GTE_3_DAYS].astype('bool')\n",
    "\n",
    "x_data['dhc_in_mm'] = x_data_pre['eus_ercp_mrcp_or_as_dhc_width_in_mm']\n",
    "x_data['clinical_data_stone_microlithiasis_sludge'] = x_data_pre['clinical_data_stone_microlithiasis_sludge']\n",
    "\n",
    "x_data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de76885-d1a1-498e-98ad-307aea421fbd",
   "metadata": {},
   "source": [
    "### Split data in training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(x_data, test_size=0.2, random_state=42)\n",
    "\n",
    "test_df_ecrp_gte_3_days = test_df[ECRP_GTE_3_DAYS]\n",
    "\n",
    "test_df_pandas = test_df.copy()\n",
    "\n",
    "# remove pass_dhc_col from test_df\n",
    "test_df = test_df.drop(columns=['dhc_in_mm'])\n",
    "train_df = train_df.drop(columns=['dhc_in_mm'])\n",
    "test_df = test_df.drop(columns=['clinical_data_stone_microlithiasis_sludge'])\n",
    "\n",
    "test_df_copy = test_df.copy()\n",
    "\n",
    "\n",
    "test_labels = test_df_copy[TARGET_COLUMN_NAME].values\n",
    "test_df_d3 = test_df_copy[ALL_FEATURE_NAMES_DAY3]\n",
    "test_df_d3.columns = test_df_d3.columns.str.replace('baseline_characteristics_', '')\n",
    "test_df_d3.columns = test_df_d3.columns.str.replace('lab_values_day_3_', '')\n",
    "model_cols = test_df_d3.columns\n",
    "test_df_d32 = test_df_d3.copy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write model columns to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cols_df = pd.DataFrame()\n",
    "model_cols_df['model_cols'] = model_cols\n",
    "model_cols_df['dtypes'] = test_df_d32.dtypes.astype('str').tolist()\n",
    "\n",
    "# Write to csv\n",
    "model_cols_df.to_csv(\"data/model_cols_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make train tf-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df[ALL_FEATURE_NAMES_ADMISSION + [\"clinical_data_stone_microlithiasis_sludge\"]]\n",
    "train_df.columns = train_df.columns.str.replace('baseline_characteristics_', '')\n",
    "train_df.columns = train_df.columns.str.replace('lab_values_admission_', '')\n",
    "#train_df = train_df[model_cols + [\"clinical_data_stone_microlithiasis_sludge\"]]\n",
    "\n",
    "train_df.dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_cols = model_cols \n",
    "select_cols = select_cols.append(pd.Index([\"clinical_data_stone_microlithiasis_sludge\"]))\n",
    "\n",
    "train_df = train_df[select_cols]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute missing values\n",
    "Uses the MICE algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e2f356",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Impute training data\n",
    "imputer = IterativeImputer(random_state=41)\n",
    "imputed = imputer.fit_transform(train_df)\n",
    "df_imputed = pd.DataFrame(imputed, columns=train_df.columns)\n",
    "\n",
    "df_imputed['has_idc'] = df_imputed['has_idc'].astype('int64')\n",
    "\n",
    "train_df_pd = df_imputed.copy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oversample minority classes (sludge and microlithiasis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define minortiy class\n",
    "is_sl_or_mic = df_imputed['clinical_data_stone_microlithiasis_sludge'].isin([2, 3, 6])\n",
    "is_sl_or_mic = is_sl_or_mic.astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "# Oversample the minority class using the SMOTE algorithm\n",
    "\n",
    "# Apply SMOTE oversampling\n",
    "smote = SMOTE(random_state=42)\n",
    "train_df_oversampled, y_train_oversampled = smote.fit_resample(df_imputed, is_sl_or_mic)\n",
    "\n",
    "print(\"Before oversampling\" + str(df_imputed.shape) + \"; after oversampling:\" + str(train_df_oversampled.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to tf dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df_oversampled[model_cols]\n",
    "train_df[\"sex\"] = train_df[\"sex\"].astype('int64')\n",
    "train_df[\"nicotine_use\"] = train_df[\"nicotine_use\"].astype('int64')\n",
    "\n",
    "train_cols = train_df.dtypes\n",
    "train_df = tfdf.keras.pd_dataframe_to_tf_dataset(train_df, label=\"has_idc\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make test tf-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = test_df[TARGET_COLUMN_NAME].values\n",
    "test_df = test_df[ALL_FEATURE_NAMES_ADMISSION]\n",
    "test_df.columns = test_df.columns.str.replace('baseline_characteristics_', '')\n",
    "test_df.columns = test_df.columns.str.replace('lab_values_admission_', '')\n",
    "test_df = test_df[model_cols]\n",
    "test_cols = test_df.dtypes\n",
    "test_df = tfdf.keras.pd_dataframe_to_tf_dataset(test_df, label=\"has_idc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cols == test_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c9e4ad-2dbb-458d-af8a-e322ea000982",
   "metadata": {},
   "source": [
    "## Train the model - random forests BASELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e01279-946c-4ef2-8a2c-554692effbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crete a tuner for model optimization on sensitivity (i.e. has_idc=1)\n",
    "\n",
    "# model_rf = tfdf.keras.GradientBoostedTreesModel()\n",
    "model_rf = tfdf.keras.RandomForestModel(\n",
    "    #tuner=tuner,\n",
    "    compute_oob_variable_importances=True,\n",
    "    )\n",
    "model_rf.fit(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14e5874",
   "metadata": {},
   "source": [
    "### Evaluate the model\n",
    "We adjust the threshold to 0.25 for sensitivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rf.compile(metrics=[\"accuracy\", \"AUC\", \"TruePositives\", \"TrueNegatives\", \"FalsePositives\", \"FalseNegatives\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b1d13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = model_rf.predict(test_df)\n",
    "preds = probs > 0.25\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "confusion_matrix(test_labels, preds)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cm = confusion_matrix(test_labels, preds)\n",
    "\n",
    "disp = ConfusionMatrixDisplay(cm)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Figure.** Confusion matrix for the test set: True positives (TP), true negatives (TN), false positives (FP), false negatives (FN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f62447-75f0-4571-b08a-5347d0fd1555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write data for analysis in R to csv\n",
    "test_df_pandas['pred_prob'] = probs\n",
    "test_df_pandas['pred_idc'] = preds\n",
    "test_df_pandas.to_csv(\"data/test_df_pandas.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ac96a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = model_rf.evaluate(test_df, return_dict=True)\n",
    "for name, value in evaluation.items():\n",
    "  print(f\"{name}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf9c870",
   "metadata": {},
   "source": [
    "#### Inspect the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e817ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_rf.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspector = model_rf.make_inspector()\n",
    "\n",
    "inspector.evaluation()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print all the variable importances\n",
    "model_rf.summary()\n",
    "\n",
    "# List the available variable importances\n",
    "print(inspector.variable_importances().keys())\n",
    "\n",
    "# Show a specific variable importance\n",
    "# Each line is: (feature name, (index of the feature), importance score)\n",
    "inspector.variable_importances()[\"MEAN_DECREASE_IN_ACCURACY\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Available variable importances:\")\n",
    "for importance in inspector.variable_importances().keys():\n",
    "  print(\"\\t\", importance)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot feature importance\n",
    "Report feature importance as Inverse mean minimum depth (INV_MEAN_MIN_DEPTH).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Mean decrease in AUC of the class 1 vs the others.\n",
    "variable_importance_metric = \"INV_MEAN_MIN_DEPTH\"\n",
    "variable_importances = inspector.variable_importances()[variable_importance_metric]\n",
    "\n",
    "# Extract the feature name and importance values.\n",
    "#\n",
    "# `variable_importances` is a list of <feature, importance> tuples.\n",
    "feature_names = [vi[0].name for vi in variable_importances]\n",
    "feature_importances = [vi[1] for vi in variable_importances]\n",
    "# The feature are ordered in decreasing importance value.\n",
    "feature_ranks = range(len(feature_names))\n",
    "\n",
    "bar = plt.barh(feature_ranks, feature_importances, label=[str(x) for x in feature_ranks])\n",
    "plt.yticks(feature_ranks, feature_names)\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "# TODO: Replace with \"plt.bar_label()\" when available.\n",
    "# Label each bar with values\n",
    "for importance, patch in zip(feature_importances, bar.patches):\n",
    "  plt.text(patch.get_x() + patch.get_width(), patch.get_y(), f\"{importance:.4f}\", va=\"top\")\n",
    "\n",
    "plt.xlabel(variable_importance_metric)\n",
    "plt.title(\"INV_MEAN_MIN_DEPTH\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write feature importances to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df['feature_names'] = feature_names\n",
    "df['feature_importances'] = feature_importances\n",
    "df['feature_ranks'] = feature_ranks\n",
    "df['variable_importance_metric'] = variable_importance_metric\n",
    "\n",
    "# Add feature type (numeric or categorical) by checing, if feature name is in part in CATEGORICAL_FEATURE_NAMES\n",
    "df['feature_type'] = 'numeric'\n",
    "df['feature_type'] = np.where(df['feature_names'].str.contains('|'.join(CATEGORICAL_FEATURE_NAMES)), 'categorical', 'numeric')\n",
    "# check for each name in df['feature_names'] if it is a substring of CATEGORICAL_FEATURE_NAMES\n",
    "for i in df['feature_names']:\n",
    "    i in ('|'.join(CATEGORICAL_FEATURE_NAMES))\n",
    "    if i in ('|'.join(CATEGORICAL_FEATURE_NAMES)):\n",
    "        df.loc[df['feature_names'] == i, 'feature_type'] = 'categorical'\n",
    "df.to_csv(\"data/feature_importances.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321a1b44",
   "metadata": {},
   "source": [
    "#### Plot the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tell dtreeviz about training data and model\n",
    "model_features = [f.name for f in model_rf.make_inspector().features()]\n",
    "\n",
    "model_features\n",
    "classes = [0, 1]\n",
    "\n",
    "train_df_pd['has_idc'] = train_df_pd['has_idc'].astype(int)\n",
    "\n",
    "viz_cmodel = dtreeviz.model(model_rf,\n",
    "                           tree_index=3,\n",
    "                           X_train=train_df_pd[model_features],\n",
    "                           y_train=train_df_pd['has_idc'],\n",
    "                           feature_names=model_features,\n",
    "                           target_name=TARGET_COLUMN_NAME,\n",
    "                           class_names=classes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot of complete model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_cmodel.view()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### First three levels of tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_cmodel.view(depth_range_to_display=[0,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Other plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train_df_pd[model_features].iloc[0]\n",
    "viz_cmodel.view(x=x, show_just_path=True, scale=.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_pd.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_cmodel.view(depth_range_to_display=[3,3], scale=1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22aae05c",
   "metadata": {},
   "source": [
    "#### Plot the training logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c064d259",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "logs = model_rf.make_inspector().training_logs()\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot([log.num_trees for log in logs], [log.evaluation.accuracy for log in logs])\n",
    "plt.xlabel(\"Number of trees\")\n",
    "plt.ylabel(\"Accuracy (out-of-bag)\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot([log.num_trees for log in logs], [log.evaluation.loss for log in logs])\n",
    "plt.xlabel(\"Number of trees\")\n",
    "plt.ylabel(\"Logloss (out-of-bag)\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test data for TFDF: Day 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the model on day 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_ecrp_gte_3_days.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset test_df_d3 for only preds == True and ecrp_gte_3_days == True\n",
    "d3 = test_df_ecrp_gte_3_days.values\n",
    "tp = (np.logical_and(d3 == True, preds == False))\n",
    "test_df_d32_keras = tfdf.keras.pd_dataframe_to_tf_dataset(test_df_d32[tp], label=\"has_idc\")\n",
    "\n",
    "\n",
    "probs_d3 = model_rf.predict(test_df_d32_keras)\n",
    "\n",
    "probs_d3 = probs_d3 > 0.3\n",
    "\n",
    "probs_d3 = probs_d3.astype(int)\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "confusion_matrix(test_labels[tp], probs_d3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cm = confusion_matrix(test_labels[tp], probs_d3)\n",
    "\n",
    "disp = ConfusionMatrixDisplay(cm)\n",
    "disp.plot()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Figure.** This figure shows the patients who did not receive an ERCP within 3 days of admission and who were classfied as 'no IDC' on admission. Note: the number of cases with IDC is very low!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = model_rf.evaluate(test_df_d32_keras, return_dict=True)\n",
    "for name, value in evaluation.items():\n",
    "  print(f\"{name}: {value:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
